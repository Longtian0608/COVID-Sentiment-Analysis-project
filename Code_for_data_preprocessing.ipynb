{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code for data preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Longtian0608/Sentiment-analysis-project/blob/main/Code_for_data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements:"
      ],
      "metadata": {
        "id": "fUNEgcI8QF0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas nltk"
      ],
      "metadata": {
        "id": "eN5Gj__-P_yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports:"
      ],
      "metadata": {
        "id": "bdPqYmIaQIoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re"
      ],
      "metadata": {
        "id": "yzDUppLAkDNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxlZ3m8Zg014"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#functions for tokenize"
      ],
      "metadata": {
        "id": "HR-igDF6ZwUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "isinstance(string.punctuation, list) "
      ],
      "metadata": {
        "id": "-SGS0BQWUWzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "#功能：提取line的核心文本，解决连词问题，匹配stopwords词库并忽略，最后还原词根\n",
        "#dix是自创的dict，解决连词（e.g. I'll -> I will)\n",
        "def tokenize(line, dix=None, remove_stopwords=False, stem=False):\n",
        "    tokens = [re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9\\']+\", '', word).lower().strip() for word in line.split()] #替换任何非A-Za-z0-9, 特殊字符@,non-whitespace,https并将其小写拆解成list\n",
        "    print(tokens)\n",
        "\n",
        "    if dix:\n",
        "        #tokens = [dix[word] if word in dix else word for word in tokens]\n",
        "        temp = []\n",
        "        for word in tokens:                 #遍历每个list里的word\n",
        "            if word in dix:                 #若word在自创的dix里,拆解成对应的value再append到list\n",
        "                sublist = dix[word].split() #(e.g. I'll -> I will)\n",
        "                for element in sublist:\n",
        "                    temp.append(element)\n",
        "            else:                           \n",
        "                temp.append(word)\n",
        "        tokens = temp\n",
        "    if remove_stopwords:                    #遇到含有stopwords里的词则忽略\n",
        "        stoppers = stopwords.words('english')\n",
        "        tokens = [word for word in tokens if word not in stoppers]\n",
        "\n",
        "    if stem:                                #还原词根的功能 (e.g. likes-->like)\n",
        "      ps = PorterStemmer()\n",
        "      tokens = [ps.stem(word) for word in tokens]\n",
        "    \n",
        "    #remove punctuations\n",
        "    #translator = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "    #tokens = [word.translate(translator) for word in tokens]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "ouVzf1xOLS1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "line = \"This is an argument with punctuations! and stopwords et stuff; But I'll get rid of these shits soon!! Will it work though?!\"\n",
        "dix = {\"i'll\": \"i will\"}\n",
        "tokenize(line, dix, True, False)"
      ],
      "metadata": {
        "id": "WZiixNrdTUSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#slandix是一个自己建立的dictionary，每个word对应一个word原型\n",
        "#我理解的功能是找出每个word和word原型对应的index？每个index对应不同word的原型\n",
        "def vocab_to_dict(vocab, slangdix=None):\n",
        "    dix1 = {}\n",
        "    dix2 = {}\n",
        "    if slangdix is None:\n",
        "        slangs = []\n",
        "    else:\n",
        "        slangs = slangdix.keys()\n",
        "    i = 0\n",
        "    for word in vocab:                         #遍历vocab每一个word\n",
        "        if word not in dix1:                   \n",
        "            if word in slangs:                 #判断word是否在dict集，如果在，找出word对应原型；如果不在，新建value并append相应index\n",
        "                origin = slangdix[word]\n",
        "                if origin in dix1:             #判断word原型是否出现过，如果有，index相同；如果没有，分别建立word和word原型并append相应index\n",
        "                    dix1[word] = dix1[origin]  \n",
        "                    \n",
        "                else:                          \n",
        "                    dix1[word] = i              \n",
        "                    dix1[origin] = i\n",
        "                    dix2[i] = origin\n",
        "                    i += 1\n",
        "            else:                             \n",
        "                dix1[word] = i\n",
        "                dix2[i] = word\n",
        "                i += 1\n",
        "    return dix1, dix2"
      ],
      "metadata": {
        "id": "bssYQKcJMt7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "vocab = ['a', 'B', 'c', 'A', 'a', 'b', 'd']\n",
        "slang = {'A':'a', 'B':'b', 'C':'c'}\n",
        "(dix1, dix2) = vocab_to_dict(vocab, slang)\n",
        "print(dix1)\n",
        "print(dix2)"
      ],
      "metadata": {
        "id": "Sh2nb5ZpZUm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##onehot\n"
      ],
      "metadata": {
        "id": "qzxqir9bZ-DV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#功能：根据上面的function得到的一个关于不同字符和其对应index的字典(dix1)，用来做one hot encoding, 将每个word转换成vector\n",
        "#rows对应有几个words，columns对应字典里有几种values（以下的例子是4个words['a', 'b', 'A', 'c']，6个index{'a': 0, 'B': 1, 'b': 1, 'c': 2, 'A': 0, 'd': 3})\n",
        "#(e.g. 'a'对应的index是0，因此他的vector就是[1,0,0,0,0,0])\n",
        "def onehot(words, dix1, length=-1):\n",
        "    if length<0:\n",
        "      x = len(words) #number of words\n",
        "    else:\n",
        "      x = length                  #x对应rows：how many words\n",
        "    values = tuple(dix1.values())\n",
        "    y = len(values)               #y对应columns：how many values in dix1\n",
        "    matrix = np.zeros((x, y)) #size of onehot matrix\n",
        "    i = 0\n",
        "    for word in words:\n",
        "        index = dix1[word]\n",
        "        matrix[i][index] = 1\n",
        "        i += 1\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "vPuHyAriM5FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#功能：通过vectors判断对应的字符 (e.g. [1,0,0,0,0,0]-> 'a')\n",
        "def onehot_to_words(words, dix2):\n",
        "    out = []\n",
        "    for word in words:\n",
        "        i, = np.where(word==1)\n",
        "        out.append(dix2[i.item()])\n",
        "    return out"
      ],
      "metadata": {
        "id": "TEUrJbGixyPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordw = ['a', 'b', 'A', 'c']\n",
        "mat = onehot(wordw, dix1, 4)\n",
        "mat"
      ],
      "metadata": {
        "id": "akz4KW65VdcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_to_words(mat, dix2)"
      ],
      "metadata": {
        "id": "-F74Ba512SRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#word to vector function\n"
      ],
      "metadata": {
        "id": "xorlvYNJYfTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('/content/gdrive/MyDrive/slgCsv/practice.xlsx')"
      ],
      "metadata": {
        "id": "xPr5kamwZezT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tokens'] = data['text'].apply(tokenize)"
      ],
      "metadata": {
        "id": "sOAujs_zaG0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['stem'] = data.apply(lambda row: tokenize(line=row['text'], remove_stopwords=True, stem=True), axis=1)"
      ],
      "metadata": {
        "id": "xshn1BAk2TGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = []\n",
        "sentences = []\n",
        "for row in data['stem']:\n",
        "    sentences.append(list(row))\n",
        "    for word in row:\n",
        "        #print(row)\n",
        "        if word not in vocab:\n",
        "            vocab.append(word)\n",
        "        else:\n",
        "            print(word)"
      ],
      "metadata": {
        "id": "_9pEKZ3KcBGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dix1, dix2 = vocab_to_dict(vocab)"
      ],
      "metadata": {
        "id": "p7uC65ibB3tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dix1)"
      ],
      "metadata": {
        "id": "gX1UTZ0EpLS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['onehot'] = data.apply(lambda x: onehot(x['stem'], dix1), axis=1)"
      ],
      "metadata": {
        "id": "L_Ob0ei-pWlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['onehot'][0].shape"
      ],
      "metadata": {
        "id": "9rKM4gIxqgm5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}